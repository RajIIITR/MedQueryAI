{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDuckGo is a privacy-focused search engine that emphasizes user anonymity and does not track its users. Its search API provides a way for developers to leverage its search capabilities programmatically. This can be particularly valuable in applications where preserving user privacy is a concern.\n",
    "\n",
    "In this context, many developers might be interested in using the DuckDuckGo search capabilities combined with Python, a versatile programming language, within the LangChain framework. LangChain refers to a framework designed for developing applications that utilize language models. It allows for the integration of various components, such as APIs and data management systems, into cohesive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we define the search_duckduckgo function that takes a query string and an optional max_results parameter to limit the number of results returned. We then utilize the ddg method from the duckduckgo_search package to get the results and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Acne - Symptoms and causes - Mayo Clinic - https://www.mayoclinic.org/diseases-conditions/acne/symptoms-causes/syc-20368047\n",
      "2: Acne: Treatment, Types, Causes, Prevention, and More - Healthline - https://www.healthline.com/health/skin/acne\n",
      "3: Acne: Types, Causes, Treatment & Prevention - Cleveland Clinic - https://my.clevelandclinic.org/health/diseases/12233-acne\n",
      "4: What is Acne? Definition & Types | NIAMS - National Institute of ... - https://www.niams.nih.gov/health-topics/acne\n",
      "5: Acne Causes: What Is Acne and Why Do I Have It? - WebMD - https://www.webmd.com/skin-problems-and-treatments/acne/understanding-acne-basics\n"
     ]
    }
   ],
   "source": [
    "def search_duckduckgo(query, max_results=5):\n",
    "    ddgs = DDGS()  # Create the DDGS instance first\n",
    "    results = ddgs.text(query, max_results=max_results)  # Then call the search method\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"{i+1}: {result['title']} - {result['href']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_duckduckgo(\"What is acne?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting our .env file which has api which we can't share or show in our main code and to link it with our code we use load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating DuckDuckGo with LangChain: Setting Up Your Chain\n",
    "Now that you can perform a simple DuckDuckGo search, we will integrate this functionality into the LangChain framework. LangChain introduces the concept of ‘chains,’ which allows for the creation of pipelines that can integrate various components, such as retrieval, transformation, and output.\n",
    "\n",
    "To set up a basic LangChain integration, we first need to define our DuckDuckGo search as part of a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a summary of information about Large Language Models (LLMs), compiled from what I consider to be the top results (based on authority, relevance, and clarity):\n",
      "\n",
      "**What are Large Language Models (LLMs)?**\n",
      "\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) model that are trained on massive amounts of text data to understand, generate, and manipulate human language. They are characterized by their:\n",
      "\n",
      "*   **Size:**  They have a huge number of parameters (billions or even trillions), which allows them to learn complex patterns in language.\n",
      "*   **Training Data:**  Trained on vast datasets of text and code scraped from the internet, books, articles, and other sources.\n",
      "*   **Transformer Architecture:**  Almost all modern LLMs are based on the transformer architecture, which enables them to process information in parallel and capture long-range dependencies in text.\n",
      "*   **Emergent Abilities:**  LLMs exhibit surprising emergent abilities, meaning capabilities that weren't explicitly programmed but arise from the scale of the model and the training data. Examples include:\n",
      "    *   Contextual understanding\n",
      "    *   Text generation\n",
      "    *   Translation\n",
      "    *   Summarization\n",
      "    *   Question answering\n",
      "    *   Code generation\n",
      "    *   Reasoning (to some extent)\n",
      "\n",
      "**Key Characteristics & Capabilities:**\n",
      "\n",
      "*   **Text Generation:**  Can generate human-quality text in various styles and formats.\n",
      "*   **Language Understanding:**  Can understand the meaning and context of text.\n",
      "*   **Translation:**  Can translate between multiple languages.\n",
      "*   **Summarization:**  Can condense large amounts of text into shorter summaries.\n",
      "*   **Question Answering:**  Can answer questions based on the information they have been trained on.\n",
      "*   **Code Generation:**  Can generate code in various programming languages.\n",
      "*   **Few-Shot Learning:** Can perform new tasks with only a few examples.\n",
      "*   **Zero-Shot Learning:** Can perform tasks without any specific training examples.\n",
      "*   **Reasoning (Limited):**  Can perform some basic reasoning tasks, but often struggles with complex or abstract reasoning.\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "*   **GPT Series (GPT-3, GPT-4) (OpenAI):**  Well-known for their text generation capabilities and used in various applications.\n",
      "*   **LaMDA (Google):**  Focuses on conversational AI.\n",
      "*   **Bard (Google):** A conversational AI service powered by LaMDA.\n",
      "*   **Llama 2 (Meta):** An open-source LLM that is free for research and commercial use.\n",
      "*   **PaLM (Google):** Pathways Language Model, known for its reasoning and multilingual capabilities.\n",
      "*   **BLOOM (BigScience):** An open-source multilingual LLM.\n",
      "\n",
      "**Applications of LLMs:**\n",
      "\n",
      "LLMs are being used in a wide range of applications, including:\n",
      "\n",
      "*   **Chatbots and Virtual Assistants:**  Powering more natural and helpful conversational experiences.\n",
      "*   **Content Creation:**  Generating articles, blog posts, marketing copy, and other types of content.\n",
      "*   **Translation Services:**  Providing more accurate and nuanced translations.\n",
      "*   **Summarization Tools:**  Helping users quickly grasp the key points of long documents.\n",
      "*   **Code Generation:**  Assisting developers in writing code more efficiently.\n",
      "*   **Search Engines:**  Improving search results and providing more informative answers.\n",
      "*   **Education:**  Personalized learning experiences and automated grading.\n",
      "*   **Customer Service:**  Automated customer support and issue resolution.\n",
      "\n",
      "**Limitations and Challenges:**\n",
      "\n",
      "Despite their impressive capabilities, LLMs have limitations:\n",
      "\n",
      "*   **Bias:**  LLMs can perpetuate and amplify biases present in their training data.\n",
      "*   **Hallucinations:**  LLMs can generate incorrect or nonsensical information that seems plausible but is not based on fact.\n",
      "*   **Lack of Common Sense:**  LLMs often lack common sense reasoning and understanding of the real world.\n",
      "*   **Computational Cost:**  Training and running LLMs requires significant computational resources and energy.\n",
      "*   **Explainability:**  It can be difficult to understand why an LLM produces a particular output.\n",
      "*   **Security Risks:**  LLMs can be used to generate malicious content, such as phishing emails and propaganda.\n",
      "*   **Copyright Issues:** The use of copyrighted material in training data raises legal and ethical questions.\n",
      "*   **Over-reliance:**  Over-dependence on LLMs can hinder critical thinking and problem-solving skills.\n",
      "\n",
      "**Ethical Considerations:**\n",
      "\n",
      "The development and deployment of LLMs raise important ethical considerations, including:\n",
      "\n",
      "*   **Fairness and Bias:**  Ensuring that LLMs are not biased against certain groups of people.\n",
      "*   **Transparency and Explainability:**  Making LLMs more transparent and explainable.\n",
      "*   **Accountability:**  Establishing accountability for the actions of LLMs.\n",
      "*   **Job Displacement:**  Addressing the potential for LLMs to displace human workers.\n",
      "*   **Misinformation:**  Preventing the use of LLMs to spread misinformation.\n",
      "\n",
      "**The Future of LLMs:**\n",
      "\n",
      "The field of LLMs is rapidly evolving.  Future directions include:\n",
      "\n",
      "*   **More efficient architectures:**  Developing more efficient LLMs that require less computational resources.\n",
      "*   **Improved reasoning abilities:**  Improving the reasoning abilities of LLMs.\n",
      "*   **Multimodal models:**  Developing LLMs that can process multiple modalities of data, such as text, images, and audio.\n",
      "*   **Personalized LLMs:**  Developing LLMs that can be personalized to individual users.\n",
      "*   **More robust safety measures:**  Developing more robust safety measures to prevent the misuse of LLMs.\n",
      "*   **Integration with other AI systems:** Combining LLMs with other AI systems to create more powerful and versatile applications.\n",
      "*   **Focus on Trustworthiness:** Moving beyond just performance to focus on reliability, safety, and alignment with human values.\n",
      "\n",
      "**In summary, LLMs are powerful AI models that are transforming the way we interact with technology. They offer many potential benefits, but it is important to be aware of their limitations and ethical considerations.**\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Define your search prompt template\n",
    "search_prompt_template = PromptTemplate.from_template(\n",
    "    \"Find information about {topic}. Provide a summary of the top results.\"\n",
    ")\n",
    "\n",
    "def gemini_search_chain(query):\n",
    "    # Initialize the Gemini model using API key from environment variable\n",
    "    # No need to set the API key explicitly as it's already in your environment\n",
    "    gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Create a chain with the prompt template and Gemini model\n",
    "    chain = LLMChain(llm=gemini, prompt=search_prompt_template)\n",
    "    \n",
    "    # Run the chain with the query\n",
    "    result = chain.run(topic=query)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    result = gemini_search_chain(\"Large Language Models\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we define a prompt template, which allows you to structure your queries for the model. The duckduckgo_search_chain function formats the prompt by substituting the {topic} with the desired query.\n",
    "\n",
    "You would then pass the formatted prompt to a language model of your choice within LangChain (here represented as Gemini), which can provide summaries, responses, or knowledge based on the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Summarizing Results\n",
    "Supposing you have integrated the search functionality into a LangChain application, the next step is to automate the retrieval and summarization of search results. After searching DuckDuckGo for a given query, you want to leverage the language model to summarize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 'LangChain framework':\n",
      "LangChain is an open-source framework designed to simplify the development, productionization, and deployment of applications powered by Large Language Models (LLMs). It facilitates the integration of LLMs into various use cases like document analysis, chatbots, and code analysis. The framework is composable and can be used standalone or integrated with other LangChain products like LangGraph (for agentic workflows) and LangSmith (for agent evaluation and observability). It provides tools for building context-aware reasoning applications and helps developers manage and scale their LLM applications.\n"
     ]
    }
   ],
   "source": [
    "# Define your search prompt template\n",
    "search_prompt_template = PromptTemplate.from_template(\n",
    "    \"Summarize the following search results about {topic}:\\n\\n{results}\"\n",
    ")\n",
    "\n",
    "def duckduckgo_search_and_summarize(query, max_results=5):\n",
    "    # Perform DuckDuckGo search\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(query, max_results=max_results)\n",
    "    \n",
    "    # Format search results for the summary\n",
    "    formatted_results = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        formatted_results += f\"{i+1}. {result['title']}: {result['href']}\\n{result.get('body', '')}\\n\\n\"\n",
    "    \n",
    "    # Initialize the Gemini model using API key from environment variables\n",
    "    gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Create a chain with the prompt template and Gemini model\n",
    "    chain = LLMChain(llm=gemini, prompt=search_prompt_template)\n",
    "    \n",
    "    # Run the chain with the search results\n",
    "    summary = chain.run(topic=query, results=formatted_results)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"LangChain framework\"\n",
    "    summary = duckduckgo_search_and_summarize(query)\n",
    "    print(f\"Summary for '{query}':\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we perform the DuckDuckGo search, compile the titles and links of the results into a summary, and format that summary for a final query to the language model, which would be responsible for generating a polished output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Enhancements\n",
    "In a real-world application, you may encounter various errors such as connection issues, invalid queries, or unexpected API responses. Effective error handling is crucial for maintaining a robust application. Below is an example of incorporating basic error handling in the search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_search_duckduckgo(query):\n",
    "    try:\n",
    "        # Create DDGS instance first\n",
    "        ddgs = DDGS()\n",
    "        # Then call the text search method\n",
    "        results = ddgs.text(query)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No results found.\"\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function captures exceptions that may arise during the API call and provides user-friendly feedback. It is advisable to expand on this by implementing logging mechanisms for better monitoring and maintenance.\n",
    "\n",
    "Additionally, consider optimizing your queries by adding parameters, such as specifying the region or the type of information you’re looking for to enhance user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are doing the compile up work of above code creating a chatbot with whom we can ask question based on the summary provided by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Conversational Search System!\n",
      "Ask a question, and I'll search for information and summarize it.\n",
      "Type 'exit' at any time to quit.\n",
      "\n",
      "\n",
      "Searching and summarizing...\n",
      "\n",
      "--- SEARCH SUMMARY ---\n",
      "Large Language Models (LLMs) are AI systems, specifically machine learning models, designed for natural language processing. They utilize deep learning techniques and neural networks with a massive number of parameters, trained on vast amounts of text data using self-supervised learning. This allows them to understand, generate, summarize, and predict text-based content. LLMs are a type of generative AI and are incredibly flexible, capable of performing diverse tasks like question answering, summarization, translation, and sentence completion. They have the potential to revolutionize content creation, search engines, and virtual assistants. Some of the most powerful LLMs are based on the Generative Pre-trained Transformer (GPT) architecture.\n",
      "\n",
      "Processing your follow-up question...\n",
      "\n",
      "--- FOLLOW-UP RESPONSE ---\n",
      "Here's the breakdown of the difference between GPT and Transformer, based on the provided information:\n",
      "\n",
      "*   **Transformer:** The Transformer is a specific neural network *architecture*. It's a design for how the model is structured, especially well-suited for handling sequences of data. The Transformer architecture is the foundation upon which many LLMs are built.\n",
      "\n",
      "*   **GPT (Generative Pre-trained Transformer):** GPT is a *specific type of Large Language Model (LLM)* that *uses* the Transformer architecture. It's a model based on transformer that excels at text generation, text completion, and a wide range of language-related tasks. It's characterized by being pre-trained on vast amounts of data, enabling it to generate coherent and contextually appropriate responses.\n",
      "\n",
      "In essence, the Transformer is the underlying architecture, while GPT is a specific LLM implementation that utilizes that architecture. You can think of it like this: the Transformer is the blueprint, and GPT is a building constructed using that blueprint.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from duckduckgo_search import DDGS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define your prompt templates\n",
    "search_prompt_template = PromptTemplate.from_template(\n",
    "    \"Summarize the following search results about {topic}:\\n\\n{results}\"\n",
    ")\n",
    "\n",
    "follow_up_prompt_template = PromptTemplate.from_template(\n",
    "    \"Based on the previous information about {original_topic}, answer this follow-up question: {follow_up_question}\\n\\n\"\n",
    "    \"Previous summary: {previous_summary}\\n\\n\"\n",
    "    \"If you need additional information to answer the follow-up, here are new search results: {new_results}\"\n",
    ")\n",
    "\n",
    "def safe_search_duckduckgo(query, max_results=5):\n",
    "    \"\"\"\n",
    "    Safely perform a DuckDuckGo search and handle any errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create DDGS instance first\n",
    "        ddgs = DDGS()\n",
    "        # Then call the text search method\n",
    "        results = ddgs.text(query, max_results=max_results)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No results found.\"\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def format_search_results(results):\n",
    "    \"\"\"\n",
    "    Format search results into a readable string.\n",
    "    \"\"\"\n",
    "    if isinstance(results, str):\n",
    "        return results  # This is an error message\n",
    "    \n",
    "    formatted_results = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        formatted_results += f\"{i+1}. {result['title']}: {result['href']}\\n{result.get('body', '')}\\n\\n\"\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def duckduckgo_search_and_summarize(query, max_results=5):\n",
    "    \"\"\"\n",
    "    Search DuckDuckGo and summarize the results using Gemini.\n",
    "    \"\"\"\n",
    "    # Perform DuckDuckGo search\n",
    "    results = safe_search_duckduckgo(query, max_results)\n",
    "    \n",
    "    # Format search results\n",
    "    formatted_results = format_search_results(results)\n",
    "    \n",
    "    # Initialize the Gemini model\n",
    "    gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Create a chain with the prompt template and Gemini model\n",
    "    chain = LLMChain(llm=gemini, prompt=search_prompt_template)\n",
    "    \n",
    "    # Run the chain with the search results\n",
    "    summary = chain.run(topic=query, results=formatted_results)\n",
    "    \n",
    "    return summary, formatted_results\n",
    "\n",
    "def handle_follow_up(follow_up_question, original_query, previous_summary, previous_results):\n",
    "    \"\"\"\n",
    "    Handle follow-up questions by searching again if needed and using context.\n",
    "    \"\"\"\n",
    "    # First, try to see if we need new information\n",
    "    combined_query = f\"{original_query} {follow_up_question}\"\n",
    "    \n",
    "    # Do a new search for the follow-up\n",
    "    new_results = safe_search_duckduckgo(combined_query)\n",
    "    formatted_new_results = format_search_results(new_results)\n",
    "    \n",
    "    # Initialize the Gemini model\n",
    "    gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Create a chain with the follow-up prompt template\n",
    "    follow_up_chain = LLMChain(llm=gemini, prompt=follow_up_prompt_template)\n",
    "    \n",
    "    # Run the chain with all available context\n",
    "    response = follow_up_chain.run(\n",
    "        original_topic=original_query,\n",
    "        follow_up_question=follow_up_question,\n",
    "        previous_summary=previous_summary,\n",
    "        new_results=formatted_new_results\n",
    "    )\n",
    "    \n",
    "    return response, formatted_new_results\n",
    "\n",
    "def conversational_handler():\n",
    "    \"\"\"\n",
    "    Main function to handle the conversation flow.\n",
    "    \"\"\"\n",
    "    print(\"Welcome to the Conversational Search System!\")\n",
    "    print(\"Ask a question, and I'll search for information and summarize it.\")\n",
    "    print(\"Type 'exit' at any time to quit.\\n\")\n",
    "    \n",
    "    original_query = None\n",
    "    previous_summary = None\n",
    "    previous_results = None\n",
    "    \n",
    "    while True:\n",
    "        # First query or new topic\n",
    "        if not original_query:\n",
    "            query = input(\"\\nWhat would you like to search for? \")\n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            print(\"\\nSearching and summarizing...\")\n",
    "            summary, results = duckduckgo_search_and_summarize(query)\n",
    "            \n",
    "            print(\"\\n--- SEARCH SUMMARY ---\")\n",
    "            print(summary)\n",
    "            \n",
    "            original_query = query\n",
    "            previous_summary = summary\n",
    "            previous_results = results\n",
    "        \n",
    "        # Follow-up questions\n",
    "        follow_up = input(\"\\nWhat else would you like to know about this topic? (or type 'new' for a new search, 'exit' to quit) \")\n",
    "        \n",
    "        if follow_up.lower() == 'exit':\n",
    "            break\n",
    "        elif follow_up.lower() == 'new':\n",
    "            original_query = None\n",
    "            previous_summary = None\n",
    "            previous_results = None\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nProcessing your follow-up question...\")\n",
    "        follow_up_response, new_results = handle_follow_up(\n",
    "            follow_up, \n",
    "            original_query, \n",
    "            previous_summary, \n",
    "            previous_results\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- FOLLOW-UP RESPONSE ---\")\n",
    "        print(follow_up_response)\n",
    "        \n",
    "        # Update the previous summary to include this follow-up\n",
    "        previous_summary += f\"\\n\\nFollow-up about {follow_up}:\\n{follow_up_response}\"\n",
    "        previous_results += new_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conversational_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the code explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Explanation of the Conversational Search System\n",
    "Let me walk through the entire codebase, explaining each component and how they work together to create a conversational search experience.\n",
    "\n",
    "\n",
    "## Imports and Setup\n",
    "\n",
    "import os <br>\n",
    "from dotenv import load_dotenv <br>\n",
    "from duckduckgo_search import DDGS <br>\n",
    "from langchain.prompts import PromptTemplate <br>\n",
    "from langchain.chains import LLMChain <br>\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI <br>\n",
    "\n",
    "#Load environment variables from .env file <br>\n",
    "load_dotenv()<br>\n",
    "\n",
    "\n",
    "This section:\n",
    "\n",
    "->Imports the necessary libraries for environment variables, DuckDuckGo search, and LangChain components<br>\n",
    "->Uses load_dotenv() to load your Gemini API key from a .env file into the environment variables<br>\n",
    "\n",
    "## Prompt Templates\n",
    "\n",
    "search_prompt_template = PromptTemplate.from_template( <br>\n",
    "    \"Summarize the following search results about {topic}:\\n\\n{results}\"\n",
    ")<br>\n",
    "\n",
    "follow_up_prompt_template = PromptTemplate.from_template( <br>\n",
    "    \"Based on the previous information about {original_topic}, answer this follow-up question: {follow_up_question}\\n\\n\" <br>\n",
    "    \"Previous summary: {previous_summary}\\n\\n\" <br>\n",
    "    \"If you need additional information to answer the follow-up, here are new search results: {new_results}\"\n",
    ")<br>\n",
    "These templates define how we'll instruct the Gemini model:\n",
    "\n",
    "->search_prompt_template: Used for initial queries to summarize search results <br>\n",
    "->follow_up_prompt_template: Used for follow-up questions, providing context from previous interactions\n",
    "\n",
    "## Function: safe_search_duckduckgo\n",
    "\n",
    "def safe_search_duckduckgo(query, max_results=5): <br>\n",
    "    \"\"\" <br>\n",
    "    Safely perform a DuckDuckGo search and handle any errors.<br>\n",
    "    \"\"\"<br>\n",
    "    try:<br>\n",
    "        # Create DDGS instance first<br>\n",
    "        ddgs = DDGS() <br>\n",
    "        # Then call the text search method <br>\n",
    "        results = ddgs.text(query, max_results=max_results) <br>\n",
    "        \n",
    "        if not results: \n",
    "            return \"No results found.\" \n",
    "        return results \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\" \n",
    "\n",
    "\n",
    "Role: This function is a wrapper around the DuckDuckGo search API that:\n",
    "\n",
    "->Creates a DDGS client <br>\n",
    "->Performs a text search with the given query <br>\n",
    "->Handles potential errors gracefully <br>\n",
    "->Returns either search results or an error message if the search fails <br>\n",
    "->Limits results to a specified number (default 5) <br>\n",
    "\n",
    "## Function: format_search_results <br>\n",
    "def format_search_results(results): <br>\n",
    "    \"\"\"<br>\n",
    "    Format search results into a readable string. <br>\n",
    "    \"\"\"<br>\n",
    "    if isinstance(results, str):<br>\n",
    "        return results  # This is an error message<br>\n",
    "    \n",
    "    formatted_results = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        formatted_results += f\"{i+1}. {result['title']}: {result['href']}\\n{result.get('body', '')}\\n\\n\"\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "Role: This function takes raw search results and converts them into a formatted string:\n",
    "\n",
    "->Checks if the input is already a string (error message)<br>\n",
    "->Creates a numbered list of search results<br>\n",
    "->Includes the title, URL, and body text (if available) for each result<br>\n",
    "->Separates results with newlines for readability<br>\n",
    "\n",
    "## Function: duckduckgo_search_and_summarize<br>\n",
    "def duckduckgo_search_and_summarize(query, max_results=5):<br>\n",
    "    \"\"\"<br>\n",
    "    Search DuckDuckGo and summarize the results using Gemini.<br>\n",
    "    \"\"\"<br>\n",
    "    # Perform DuckDuckGo search<br>\n",
    "    results = safe_search_duckduckgo(query, max_results)<br>\n",
    "    \n",
    "    # Format search results \n",
    "\n",
    "    formatted_results = format_search_results(results)\n",
    "    \n",
    "    # Initialize the Gemini model\n",
    "\n",
    "    gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Create a chain with the prompt template and Gemini model\n",
    "    \n",
    "    chain = LLMChain(llm=gemini, prompt=search_prompt_template)\n",
    "    \n",
    "    # Run the chain with the search results\n",
    "\n",
    "    summary = chain.run(topic=query, results=formatted_results)\n",
    "    \n",
    "    return summary, formatted_results \n",
    "\n",
    "\n",
    "Role: This function combines search and summarization:\n",
    "\n",
    "->Calls safe_search_duckduckgo to get search results<br>\n",
    "->Formats the results using format_search_results<br>\n",
    "->Initializes the Gemini model<br>\n",
    "->Creates a LangChain chain with the search prompt template<br>\n",
    "->Runs the chain to generate a summary of the search results<br>\n",
    "->Returns both the summary and the formatted results (to maintain context for follow-ups)<br>\n",
    "\n",
    "## Function: handle_follow_up <br>\n",
    "def handle_follow_up(follow_up_question, original_query, previous_summary, previous_results):<br>\n",
    "    \"\"\"<br>\n",
    "    Handle follow-up questions by searching again if needed and using context.<br>\n",
    "    \"\"\"<br>\n",
    "    # First, try to see if we need new information<br>\n",
    "    combined_query = f\"{original_query} {follow_up_question}\"<br>\n",
    "    \n",
    "    # Do a new search for the follow-up \n",
    "\n",
    "    new_results = safe_search_duckduckgo(combined_query)\n",
    "\n",
    "    formatted_new_results = format_search_results(new_results)\n",
    "    \n",
    "    # Initialize the Gemini model\n",
    "\n",
    "    gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Create a chain with the follow-up prompt template\n",
    "\n",
    "    follow_up_chain = LLMChain(llm=gemini, prompt=follow_up_prompt_template)\n",
    "    \n",
    "    # Run the chain with all available context\n",
    "    \n",
    "    response = follow_up_chain.run(\n",
    "        original_topic=original_query,\n",
    "        follow_up_question=follow_up_question,\n",
    "        previous_summary=previous_summary,\n",
    "        new_results=formatted_new_results\n",
    "    )\n",
    "    \n",
    "    return response, formatted_new_results\n",
    "\n",
    "Role: This function processes follow-up questions by:\n",
    "\n",
    "->Creating a combined query from the original query and follow-up question<br>\n",
    "->Performing a new search with this combined query<br>\n",
    "->Initializing the Gemini model<br>\n",
    "->Creating a LangChain chain with the follow-up prompt template<br>\n",
    "->Running the chain with all available context (original topic, follow-up question, previous summary, and new results)<br>\n",
    "->Returning the response and the new formatted results<br>\n",
    "\n",
    "## Function: conversational_handler <br>\n",
    "def conversational_handler(): <br>\n",
    "    \"\"\"<br>\n",
    "    Main function to handle the conversation flow.<br>\n",
    "    \"\"\"<br>\n",
    "    print(\"Welcome to the Conversational Search System!\")<br>\n",
    "    print(\"Ask a question, and I'll search for information and summarize it.\")<br>\n",
    "    print(\"Type 'exit' at any time to quit.\\n\")<br>\n",
    "    \n",
    "    original_query = None\n",
    "\n",
    "    previous_summary = None\n",
    "    \n",
    "    previous_results = None\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # First query or new topic\n",
    "        \n",
    "        if not original_query:\n",
    "\n",
    "            query = input(\"\\nWhat would you like to search for? \")\n",
    "            \n",
    "            if query.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            print(\"\\nSearching and summarizing...\")\n",
    "\n",
    "            summary, results = duckduckgo_search_and_summarize(query)\n",
    "            \n",
    "            print(\"\\n--- SEARCH SUMMARY ---\")\n",
    "\n",
    "            print(summary)\n",
    "            \n",
    "            original_query = query\n",
    "\n",
    "            previous_summary = summary\n",
    "\n",
    "            previous_results = results\n",
    "        \n",
    "        # Follow-up questions\n",
    "\n",
    "        follow_up = input(\"\\nWhat else would you like to know about this topic? (or type 'new' for a new search, 'exit' to quit) \")\n",
    "        \n",
    "        if follow_up.lower() == 'exit':\n",
    "            break\n",
    "        elif follow_up.lower() == 'new':\n",
    "\n",
    "            original_query = None\n",
    "\n",
    "            previous_summary = None\n",
    "\n",
    "            previous_results = None\n",
    "\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nProcessing your follow-up question...\")\n",
    "\n",
    "        follow_up_response, new_results = handle_follow_up(\n",
    "            follow_up, \n",
    "            original_query, \n",
    "            previous_summary, \n",
    "            previous_results\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- FOLLOW-UP RESPONSE ---\")\n",
    "        \n",
    "        print(follow_up_response)\n",
    "        \n",
    "        # Update the previous summary to include this follow-up\n",
    "\n",
    "        previous_summary += f\"\\n\\nFollow-up about {follow_up}:\\n{follow_up_response}\"\n",
    "\n",
    "        previous_results += new_results\n",
    "\n",
    "Role: This is the main function that orchestrates the entire conversational experience:\n",
    "\n",
    "->Welcomes the user and explains how to use the system<br>\n",
    "->Initializes variables to track conversation state<br>\n",
    "->Enters a loop to handle interactions<br>\n",
    "->For initial queries:<br>\n",
    "    ->Gets user input<br>\n",
    "    ->Checks for exit command<br>\n",
    "    ->Calls duckduckgo_search_and_summarize<br>\n",
    "    ->Displays the summary<br>\n",
    "    ->Stores the query, summary, and results for context<br>\n",
    "\n",
    "\n",
    "->For follow-up queries:<br>\n",
    "    ->Gets user input<br>\n",
    "    ->Handles exit/new commands<br>\n",
    "    ->Calls handle_follow_up with the stored context<br>\n",
    "    ->Displays the response<br>\n",
    "    ->Updates the stored context with the new information<br>\n",
    "->Allows the user to start new topics or exit at any time\n",
    "\n",
    "## Main Block <br>\n",
    "if __name__ == \"__main__\":<br>\n",
    "    conversational_handler()\n",
    "\n",
    "Role: This ensures that the conversational_handler function is called when the script is run directly (not imported).<br>\n",
    "## Overall Architecture and Flow\n",
    "\n",
    "The system follows this flow:\n",
    "\n",
    "1.User enters an initial query <br>\n",
    "2.System searches DuckDuckGo and summarizes results with Gemini<br>\n",
    "3.User asks follow-up questions<br>\n",
    "4.System uses context from previous interactions plus new searches to answer<br>\n",
    "5.Conversation continues until user exits or starts a new topic\n",
    "\n",
    "The key innovation here is maintaining state between interactions, allowing for a more natural conversation that builds upon previous context. This makes the system more intuitive than having to restate the entire query with each interaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
